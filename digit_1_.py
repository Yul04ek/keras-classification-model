# -*- coding: utf-8 -*-
"""Копія записника "digit_1"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/141i_0SmJAyrVyBfVPXKHVv4_wHAOJEp6
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

digit_recognizer_path = kagglehub.competition_download('digit-recognizer')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df_test = pd.read_csv("/kaggle/input/digit-recognizer/test.csv")
df_train = pd.read_csv("/kaggle/input/digit-recognizer/train.csv")
sample_submission = pd.read_csv("/kaggle/input/digit-recognizer/sample_submission.csv")

df_train.head()

sample_submission.head()

df_train.isna().sum().sort_values(ascending = False)

df_train.describe()

df_train[(df_train.iloc[:, 1: ]==0).all(axis=1)]

df_train['label'].value_counts().sort_index().plot(kind='bar', color='skyblue')
plt.title('Розподіл класів у df_train')

import random

def draw_digit (label,pixels):

    image = pixels.reshape(28, 28)        # Convert to 28x28

    plt.imshow(image, cmap='gray')
    plt.title(f"Label: {label}")
    plt.axis('off')
    plt.show()

random_indices = random.sample(range(len(df_train)), 3)

for i in random_indices:
    draw_digit(df_train.iloc[i, 0], df_train.iloc[i, 1:].values)

import keras
from tensorflow.keras.layers import Dropout
from tensorflow.keras import regularizers
from keras import layers

# Determining the number of features
num_features = df_train.shape[1]-1
print(num_features)

# Model creation
def uncompiled_model():
    model = keras.Sequential([keras.Input(shape=(num_features,)),
                            layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
                        Dropout(0.3),
                            layers.Dense(128, activation="relu", name="hidden_layer"),
                        Dropout(0.2),
                         layers.Dense(10, activation="softmax", name='output_layer')
                         ])
    return model
uncompiled_model().summary()

# Model compilation
def get_compiled_model (optimizer="adam",    loss='sparse_categorical_crossentropy', metrics=["sparse_categorical_accuracy"]):
    model = uncompiled_model()
    model.compile(optimizer=optimizer,  loss=loss,  metrics=metrics )
    return model

from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
#  Data
X = df_train.drop('label',axis=1)
y= df_train['label']
X_train, X_test, y_train, y_test = train_test_split(X, y , random_state = None, test_size=0.2)

#  EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)
#  Model training
model = get_compiled_model()
history = model.fit(
    X_train, y_train,
    validation_split=0.2,       #  required for val_loss
    batch_size=64,
    epochs=45,
    callbacks=[early_stop]      #  add EarlyStopping
)

#  Rating
results = model.evaluate(X_test, y_test)

print("Loss:", results[0])
print("Accuracy:", results[1])

def generate_prediction(model, x_data, save_csv=False, filename='submission.csv'):
    """
   Predict the model on x_data (can be X_test or df_test).

  Parameters:
  model: trained Keras model
  x_data: DataFrame or pixel array
  save_csv: whether to save the result to CSV
  filename: filename to save
  Returns:
  pred_labels: array of predicted labels
    """
    # Convert to NumPy if it is a DataFrame
    if isinstance(x_data, pd.DataFrame):
        x_data = x_data.to_numpy()

    # predict
    y_pred_probs = model.predict(x_data)
    y_pred_labels = y_pred_probs.argmax(axis=1)

    # If necessary, save CSV
    if save_csv:
        submission = pd.DataFrame({
            'ImageId': range(1, len(y_pred_labels) + 1),
            'Label': y_pred_labels
        })
        submission.to_csv(filename, index=False)
        print(f"✅ Submission saved to {filename}")

    return y_pred_labels

predicted_digits = generate_prediction(model, X_test)

print(predicted_digits)

from sklearn.metrics import classification_report

y_pred = model.predict(X_test).argmax(axis=1)
print(classification_report(y_test, y_pred))

random_indices = random.sample(range(len(X_test)), 3)

for i in random_indices:
    draw_digit(y_pred[i], X_test.iloc[i].values)

y = generate_prediction(model, df_test, save_csv=True)
y

import datetime
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
model.save(f"my_model_{timestamp}.h5")